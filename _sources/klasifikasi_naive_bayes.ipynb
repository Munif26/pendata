{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM2K2KvTao8S4okQqE2BLxi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Klasifikasi Naive Bayes**"],"metadata":{"id":"0EV4SfGA6MGs"}},{"cell_type":"markdown","source":["# **Apa itu naive bayes...?**"],"metadata":{"id":"iFNOtfb36Z1X"}},{"cell_type":"markdown","source":["Naïve Bayes adalah salah satu algoritma klasifikasi dalam pembelajaran mesin yang termasuk dalam metode pembelajaran terbimbing (*supervised learning*). Algoritma ini sering digunakan dalam berbagai tugas klasifikasi seperti klasifikasi teks, filtering spam, analisis sentimen, dan berbagai aplikasi lainnya. Naïve Bayes didasarkan pada prinsip probabilitas dan menggunakan Teorema Bayes sebagai dasar perhitungannya. Teorema Bayes memungkinkan kita untuk menghitung kemungkinan suatu kejadian berdasarkan informasi yang telah diketahui sebelumnya. Dalam konteks klasifikasi, algoritma ini menghitung probabilitas suatu kategori berdasarkan fitur-fitur yang diberikan dalam data.  \n","\n","Istilah *Naïve* atau *Naif* dalam Naïve Bayes digunakan karena algoritma ini mengasumsikan bahwa setiap fitur dalam data bersifat independen satu sama lain dalam menentukan kategori akhir. Dengan kata lain, algoritma ini menganggap bahwa tidak ada hubungan antar fitur, meskipun dalam kenyataannya mungkin ada keterkaitan. Sebagai contoh, dalam klasifikasi teks, Naïve Bayes mengasumsikan bahwa kemunculan sebuah kata dalam dokumen tidak dipengaruhi oleh kata lain, padahal dalam bahasa alami, kata-kata sering kali memiliki keterkaitan. Meskipun asumsi ini sederhana dan sering kali tidak realistis, algoritma Naïve Bayes tetap bekerja dengan baik dalam banyak kasus.    \n","\n","Naïve Bayes memiliki beberapa keunggulan, di antaranya adalah kecepatan dan efisiensinya dalam memproses dataset yang besar, kemudahan implementasi dengan perhitungan probabilitas sederhana, serta kemampuannya bekerja dengan baik pada data kategorikal maupun teks. Selain itu, algoritma ini tidak membutuhkan banyak data latih untuk memberikan hasil yang baik. Namun, terdapat pula beberapa kelemahan, seperti asumsi independensi fitur yang sering kali tidak sesuai dengan kondisi nyata, kurangnya akurasi jika fitur saling bergantung satu sama lain, serta sensitivitas terhadap data yang belum pernah ditemui, yang dapat menyebabkan kesalahan jika ada kata atau fitur baru yang tidak muncul dalam data pelatihan.  \n","\n","Secara keseluruhan, meskipun memiliki keterbatasan, algoritma Naïve Bayes tetap menjadi salah satu metode klasifikasi yang populer, terutama dalam pengolahan teks dan analisis data berbasis probabilitas. Kecepatan, kesederhanaan, dan efektivitasnya dalam menangani berbagai tugas klasifikasi menjadikannya pilihan yang kuat dalam berbagai aplikasi pembelajaran mesin."],"metadata":{"id":"BvvmSev16c2x"}},{"cell_type":"markdown","source":["# **Berikut adalah rumus naive bayes...!**"],"metadata":{"id":"zNjvGvxH7Y6S"}},{"cell_type":"markdown","source":["**Naïve Bayes Classifier**  \n","\n","Naïve Bayes adalah metode klasifikasi berdasarkan **Teorema Bayes**. Rumus dasar dari Teorema Bayes adalah:  \n","\n","$$\n","P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n","$$\n","\n","Di mana:  \n","- \\( P(C|X) \\) = Probabilitas kelas \\( C \\) berdasarkan fitur \\( X \\) (*posterior probability*).  \n","- \\( P(X|C) \\) = Probabilitas fitur \\( X \\) muncul dalam kategori \\( C \\) (*likelihood*).  \n","- \\( P(C) \\) = Probabilitas awal dari kelas \\( C \\) (*prior probability*).  \n","- \\( P(X) \\) = Probabilitas fitur \\( X \\) dalam seluruh dataset (*evidence*).  \n","\n","Karena dalam Naïve Bayes kita mengasumsikan fitur-fitur **bersifat independen**, maka probabilitas gabungan dari beberapa fitur \\( X_1, X_2, ..., X_n \\) dapat dihitung sebagai:  \n","\n","$$\n","P(C|X_1, X_2, ..., X_n) = \\frac{P(C) \\cdot P(X_1|C) \\cdot P(X_2|C) \\cdots P(X_n|C)}{P(X_1) \\cdot P(X_2) \\cdots P(X_n)}\n","$$\n","\n","Karena **\\( P(X) \\)** sama untuk semua kelas dalam klasifikasi, kita cukup menghitung pembilangnya saja:\n","\n","$$\n","P(C|X_1, X_2, ..., X_n) \\propto P(C) \\cdot P(X_1|C) \\cdot P(X_2|C) \\cdots P(X_n|C)\n","$$\n","\n","Jika terdapat kemungkinan kata atau fitur belum pernah muncul dalam data latih, maka kita menggunakan **Laplace Smoothing** untuk menghindari probabilitas nol:\n","\n","$$\n","P(X_i|C) = \\frac{\\text{jumlah } X_i \\text{ dalam } C + 1}{\\text{total kata dalam } C + |V|}\n","$$\n","\n","Di mana:  \n","- \\( |V| \\) adalah jumlah total kata unik dalam seluruh dataset.  \n","- \\( 1 \\) adalah nilai smoothing untuk menghindari pembagian dengan nol.  \n","\n","## **Contoh Perhitungan**  \n","\n","Misalkan kita memiliki dataset dengan dua kategori: **Spam** dan **Non-Spam**, dengan informasi berikut:  \n","\n","| Kata dalam Email  | Spam (\\( C_1 \\)) | Non-Spam (\\( C_2 \\)) |\n","|------------------|-----------|---------------|\n","| Promo           | 3         | 1             |\n","| Diskon          | 2         | 1             |\n","| Transfer        | 1         | 2             |\n","| Sekarang        | 2         | 1             |\n","| **Total Kata**  | 8         | 5             |\n","\n","Jika kita ingin menghitung probabilitas bahwa email **\"Promo Diskon\"** adalah **Spam**, kita gunakan rumus:  \n","\n","### **1. Prior Probabilities**  \n","$$\n","P(\\text{Spam}) = \\frac{\\text{Jumlah Email Spam}}{\\text{Total Email}}\n","$$\n","\n","### **2. Likelihood dengan Laplace Smoothing**  \n","$$\n","P(\\text{Promo} | \\text{Spam}) = \\frac{3 + 1}{8 + 4} = \\frac{4}{12}\n","$$  \n","$$\n","P(\\text{Diskon} | \\text{Spam}) = \\frac{2 + 1}{8 + 4} = \\frac{3}{12}\n","$$  \n","\n","### **3. Kalkulasi Probabilitas Naïve Bayes**  \n","$$\n","P(\\text{Spam} | \\text{Promo, Diskon}) \\propto P(\\text{Spam}) \\times P(\\text{Promo} | \\text{Spam}) \\times P(\\text{Diskon} | \\text{Spam})\n","$$"],"metadata":{"id":"Q5WHa6dZ7mrR"}},{"cell_type":"markdown","source":["# **Jenis-Jenis Naive Bayes...!**"],"metadata":{"id":"pFx7DpDC-pgS"}},{"cell_type":"markdown","source":["Naive Bayes merupakan metode klasifikasi yang digunakan untuk berbagai jenis data. Berikut adalah penjelasan mengenai jenis-jenis Naive Bayes yang dapat digunakan sesuai dengan tipe data yang dimiliki:\n","\n","**1. Gaussian Naive Bayes**\n","\n","Gaussian Naive Bayes digunakan untuk data kontinu, seperti pengukuran fisik (misalnya, tinggi badan atau berat badan). Metode ini mengasumsikan bahwa fitur-fitur data mengikuti distribusi normal (Gaussian). Probabilitas suatu fitur dihitung menggunakan distribusi normal dengan rata-rata (\\( \\mu_C \\)) dan varians (\\( \\sigma_C^2 \\)) dari kelas tersebut.\n","\n","Formula probabilitasnya adalah:\n","\n","$$\n","P(X_i | C) = \\frac{1}{\\sqrt{2 \\pi \\sigma_C^2}} \\exp \\left( - \\frac{(X_i - \\mu_C)^2}{2\\sigma_C^2} \\right)\n","$$\n","\n","- \\( X_i \\) = nilai fitur ke-\\( i \\) pada data\n","- \\( \\mu_C \\) = rata-rata fitur pada kelas \\( C \\)\n","- \\( \\sigma_C^2 \\) = varians fitur pada kelas \\( C \\)\n","\n","**Contoh:**  \n","Jika kita ingin mengklasifikasikan seseorang sebagai \"tinggi\" atau \"pendek\" berdasarkan tinggi badan, kita bisa menggunakan Gaussian Naïve Bayes dengan distribusi normal untuk fitur tinggi badan pada masing-masing kategori.\n","\n","\n","**2. Multinomial Naive Bayes**\n","\n","Multinomial Naive Bayes paling cocok digunakan untuk data diskrit, seperti frekuensi kemunculan fitur dalam dokumen. Misalnya, dalam klasifikasi teks, kita menghitung probabilitas kata-kata yang muncul dalam dokumen tertentu. Formula probabilitasnya adalah:\n","\n","$$\n","P(X_i | C) = \\frac{n_{i,C} + \\alpha}{N_C + \\alpha \\cdot V}\n","$$\n","\n","- \\( n_{i,C} \\) = jumlah kemunculan kata \\( i \\) dalam kelas \\( C \\)\n","- \\( N_C \\) = jumlah total kata dalam kelas \\( C \\)\n","- \\( \\alpha \\) = parameter smoothing (biasanya \\( \\alpha = 1 \\) untuk Laplace smoothing)\n","- \\( V \\) = jumlah total kata unik dalam seluruh dataset\n","\n","**Contoh:**  \n","Jika kita ingin mengklasifikasikan email sebagai \"Spam\" atau \"Non-Spam\", kita dapat menghitung probabilitas kata \"diskon\" muncul dalam email spam dibandingkan dengan non-spam menggunakan Multinomial Naïve Bayes.\n","\n","\n","\n","**3. Bernoulli Naive Bayes**\n","\n","Bernoulli Naive Bayes digunakan untuk data biner, di mana fitur hanya memiliki dua nilai kemungkinan: ada (1) atau tidak ada (0). Probabilitas suatu fitur \\( X_i \\) muncul dalam kelas \\( C \\) dihitung dengan rumus berikut:\n","\n","$$\n","P(X_i = 1 | C) = \\frac{n_{i,C} + \\alpha}{N_C + \\alpha}\n","$$\n","\n","- \\( n_{i,C} \\) = jumlah kejadian di mana fitur \\( X_i \\) ada dalam kelas \\( C \\)\n","- \\( N_C \\) = jumlah total data dalam kelas \\( C \\)\n","- \\( \\alpha \\) = parameter smoothing (biasanya \\( \\alpha = 1 \\) untuk Laplace smoothing)\n","\n","**Contoh:**  \n","Misalnya, dalam filter spam, kita dapat memeriksa apakah kata \"diskon\" ada dalam email (1) atau tidak (0) dan mengklasifikasikan email tersebut sebagai spam atau tidak spam menggunakan Bernoulli Naïve Bayes.\n","\n","\n","**Ringkasan Formula Probabilitas**\n","\n","1. **Gaussian Naive Bayes**:\n","   $$\n","   P(X_i | C) = \\frac{1}{\\sqrt{2 \\pi \\sigma_C^2}} \\exp \\left( - \\frac{(X_i - \\mu_C)^2}{2\\sigma_C^2} \\right)\n","   $$\n","\n","2. **Multinomial Naive Bayes**:\n","   $$\n","   P(X_i | C) = \\frac{n_{i,C} + \\alpha}{N_C + \\alpha \\cdot V}\n","   $$\n","\n","3. **Bernoulli Naive Bayes**:\n","   $$\n","   P(X_i = 1 | C) = \\frac{n_{i,C} + \\alpha}{N_C + \\alpha}\n","   $$\n","\n","Dengan rumus-rumus ini, Anda dapat menghitung probabilitas untuk setiap jenis Naive Bayes sesuai dengan data yang Anda miliki dan menentukan kelas yang paling mungkin.\n"],"metadata":{"id":"du4TDTiJ9f-B"}},{"cell_type":"markdown","source":["# **Berikut Perbandingan Perhitungan Data Naive Bayes...!**"],"metadata":{"id":"REgOCXTH_LdI"}},{"cell_type":"markdown","source":["Perbandingan yang dilakukan dalam penelitian ini menggunakan dataset Iris yang telah kita kumpulkan pada pertemuan sebelumnya, sehingga mempermudah dalam memperoleh data yang diperlukan. Pada langkah pertama, kita akan menghubungkan dataset Iris tersebut dengan database agar dapat memproses data lebih lanjut. Selanjutnya, perhitungan Naive Bayes dilakukan menggunakan data yang belum diproses, termasuk data yang mengandung outliers atau data kotor. Setelah itu, kita akan membersihkan data tersebut dengan menerapkan teknik Local Outlier Factor (LOF) yang telah dibahas sebelumnya untuk mendeteksi dan menghilangkan outliers dari dataset. Setelah data dibersihkan, kita akan kembali melakukan perhitungan menggunakan Naive Bayes pada data yang sudah bersih dan bebas dari outliers. Pada bagian akhir, kita akan menyajikan hasil dari kedua perhitungan tersebut, yakni perhitungan dengan data kotor dan data yang telah dibersihkan. Hasil tersebut kemudian akan dianalisis untuk mengungkapkan kesimpulan mengenai pengaruh outliers terhadap akurasi dan kinerja algoritma Naive Bayes."],"metadata":{"id":"vZs5R6b8_UII"}},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4T-89zV4ZIl","executionInfo":{"status":"ok","timestamp":1743039199709,"user_tz":-420,"elapsed":22467,"user":{"displayName":"23-177 Muhammad Hanif","userId":"11387158830586806132"}},"outputId":"20fe6529-93cc-4470-ee63-89a1ab399b9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pymysql in /usr/local/lib/python3.11/dist-packages (1.1.1)\n","Requirement already satisfied: psycopg2 in /usr/local/lib/python3.11/dist-packages (2.9.10)\n","Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.11/dist-packages (2.0.39)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.12.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","PostgreSQL 16.8 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 14.2.1 20240912 (Red Hat 14.2.1-3), 64-bit\n","\n","Berikut ini adalah hasil dari data yang diambil:\n","\n","      id           class  petal_length  petal_width  sepal length  sepal width\n","0      1     Iris-setosa          86.4         70.0          20.1         30.5\n","1      2     Iris-setosa           1.4          0.2           4.9          3.0\n","2      3     Iris-setosa           1.3          0.2           4.7          3.2\n","3      4     Iris-setosa           1.5          0.2           4.6          3.1\n","4      5     Iris-setosa           1.4          0.2           5.0          3.6\n","..   ...             ...           ...          ...           ...          ...\n","145  146  Iris-virginica           5.2          2.3           6.7          3.0\n","146  147  Iris-virginica           5.0          1.9           6.3          2.5\n","147  148  Iris-virginica           5.2          2.0           6.5          3.0\n","148  149  Iris-virginica           5.4          2.3           6.2          3.4\n","149  150  Iris-virginica           5.1          1.8           5.9          3.0\n","\n","[150 rows x 6 columns]\n"]}],"source":["# Mengkoneksi database dengan python\n","# Install library\n","!pip install pymysql\n","!pip install psycopg2\n","!pip install sqlalchemy\n","!pip install pandas\n","\n","# Import library\n","import pymysql\n","import psycopg2\n","import pandas as pd\n","from sqlalchemy import create_engine\n","\n","# for mysql\n","timeout = 10\n","connection = pymysql.connect(\n","    charset=\"utf8mb4\",\n","    connect_timeout=timeout,\n","    cursorclass=pymysql.cursors.DictCursor,\n","    db=\"defaultdb\",\n","    host=\"mysql-726cd75-mysqlpendata-11.h.aivencloud.com\",\n","    password=\"AVNS_LHA80D-LNsKI6wncjfc\",\n","    read_timeout=timeout,\n","    port=20734,\n","    user=\"avnadmin\",\n","    write_timeout=timeout,\n",")\n","mysql_engine = create_engine(\"mysql+pymysql://avnadmin:AVNS_LHA80D-LNsKI6wncjfc@mysql-726cd75-mysqlpendata-11.h.aivencloud.com:20734/defaultdb\")\n","\n","\n","\n","# for postgre\n","def main():\n","    conn = psycopg2.connect('postgres://avnadmin:AVNS__Y6I8K0T7rSnwnRgE1U@pg-3266d3cf-postgresqlpendata-11.h.aivencloud.com:20817/defaultdb?sslmode=require')\n","\n","    query_sql = 'SELECT VERSION()'\n","\n","    cur = conn.cursor()\n","    cur.execute(query_sql)\n","\n","    version = cur.fetchone()[0]\n","    print(version)\n","\n","if __name__ == \"__main__\":\n","    main()\n","postgres_engine = create_engine(\"postgresql+psycopg2://avnadmin:AVNS__Y6I8K0T7rSnwnRgE1U@pg-3266d3cf-postgresqlpendata-11.h.aivencloud.com:20817/defaultdb\")\n","\n","# Ambil data dari MySQL\n","mysql_query = \"SELECT * FROM iris_data\"\n","mysql_df = pd.read_sql(mysql_query, mysql_engine)\n","\n","# Ambil data dari PostgreSQL\n","pg_query = 'SELECT * FROM postgre'\n","pg_df = pd.read_sql(pg_query, postgres_engine)\n","\n","merge_df = pd.merge(mysql_df, pg_df, left_on=\"id\", right_on='id', how='outer')\n","\n","# Menampilkan data yang diambil database dan ditampilkan dalam bentuk tabel\n","selected_columns = [\"id\", \"class\", \"petal_length\", \"petal_width\", \"sepal length\", \"sepal width\"]\n","filtered_df = merge_df[selected_columns]\n","\n","print()\n","print(\"Berikut ini adalah hasil dari data yang diambil:\")\n","print()\n","print(filtered_df)"]},{"cell_type":"markdown","source":["## **Berikut adalah Membandingkan Data dengan Outliers...!**"],"metadata":{"id":"ENTC7QQmPIT9"}},{"cell_type":"code","source":["# Import yang diperlukan\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# Membuat DataFrame dengan data yang telah disiapkan\n","data = pd.DataFrame({\n","    'id': range(1, 151),\n","    'class': ['Iris-setosa'] * 50 + ['Iris-versicolor'] * 50 + ['Iris-virginica'] * 50,\n","    'petal_length': filtered_df['petal_length'],\n","    'petal_width': filtered_df['petal_width'],\n","    'sepal length': filtered_df['sepal length'],\n","    'sepal width': filtered_df['sepal width']\n","})\n","\n","# === Data Preprocessing ===\n","data = data.drop(columns=['id'])  # Hapus kolom ID karena tidak diperlukan\n","X = data.drop(columns=['class'])  # Fitur (features)\n","y = data['class']  # Label (target)\n","\n","# === Membagi Data menjadi Training dan Testing (80%-20%) ===\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# === Melatih Model Naïve Bayes ===\n","model = GaussianNB()\n","model.fit(X_train, y_train)\n","\n","# === Prediksi & Evaluasi ===\n","y_pred = model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(\"Akurasi Model Naïve Bayes:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pg4tPBoXPRXs","executionInfo":{"status":"ok","timestamp":1743041779838,"user_tz":-420,"elapsed":162,"user":{"displayName":"23-177 Muhammad Hanif","userId":"11387158830586806132"}},"outputId":"8808dff5-0865-4d48-db60-a5bb742011b5"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Akurasi Model Naïve Bayes: 1.0\n"]}]},{"cell_type":"markdown","source":["## **Berikut adalah Membandingkan Data tanpa Outliers...!**"],"metadata":{"id":"5VtKQxuTPwRa"}},{"cell_type":"markdown","source":["**A. PROSES FILTERING**"],"metadata":{"id":"2XW_EcUbUx4x"}},{"cell_type":"code","source":["# Pastikan kolom 'class' ada sebelum menghapusnya\n","if 'class' in data.columns:\n","    data = data.drop(columns=['class'])  # Hapus kolom 'class' jika ada\n","\n","# LOF untuk mendeteksi outlier\n","lof_model = LocalOutlierFactor(n_neighbors=3, metric='euclidean')\n","\n","lof_labels = lof_model.fit_predict(data)  # -1 = outlier, 1 = inlier\n","lof_values = -lof_model.negative_outlier_factor_\n","\n","# Tampilkan LOF dan label untuk setiap data point\n","for i in range(len(data)):\n","    print(f\"Indeks-{i} → LOF: {lof_values[i]:.4f}, Label: {'Outlier' if lof_labels[i] == -1 else 'Inlier'}\")\n","\n","filtered_data = data[lof_labels == 1]  # Hanya simpan data yang bukan outlier"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BSjpjdW2Wvak","executionInfo":{"status":"ok","timestamp":1743043601698,"user_tz":-420,"elapsed":360,"user":{"displayName":"23-177 Muhammad Hanif","userId":"11387158830586806132"}},"outputId":"d6c798f6-6e3d-4e82-f39c-aa09dc4aad9b"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Indeks-0 → LOF: 135.5942, Label: Outlier\n","Indeks-1 → LOF: 1.0776, Label: Inlier\n","Indeks-2 → LOF: 1.0697, Label: Inlier\n","Indeks-3 → LOF: 1.0050, Label: Inlier\n","Indeks-4 → LOF: 1.0453, Label: Inlier\n","Indeks-5 → LOF: 1.2883, Label: Inlier\n","Indeks-6 → LOF: 1.0516, Label: Inlier\n","Indeks-7 → LOF: 0.9114, Label: Inlier\n","Indeks-8 → LOF: 1.0996, Label: Inlier\n","Indeks-9 → LOF: 0.9596, Label: Inlier\n","Indeks-10 → LOF: 1.2623, Label: Inlier\n","Indeks-11 → LOF: 1.0550, Label: Inlier\n","Indeks-12 → LOF: 0.9596, Label: Inlier\n","Indeks-13 → LOF: 1.0592, Label: Inlier\n","Indeks-14 → LOF: 1.1404, Label: Inlier\n","Indeks-15 → LOF: 1.1932, Label: Inlier\n","Indeks-16 → LOF: 1.1637, Label: Inlier\n","Indeks-17 → LOF: 1.0582, Label: Inlier\n","Indeks-18 → LOF: 1.3447, Label: Inlier\n","Indeks-19 → LOF: 0.9854, Label: Inlier\n","Indeks-20 → LOF: 1.6460, Label: Outlier\n","Indeks-21 → LOF: 1.0483, Label: Inlier\n","Indeks-22 → LOF: 2.0784, Label: Outlier\n","Indeks-23 → LOF: 1.2242, Label: Inlier\n","Indeks-24 → LOF: 1.4252, Label: Inlier\n","Indeks-25 → LOF: 1.1547, Label: Inlier\n","Indeks-26 → LOF: 1.2059, Label: Inlier\n","Indeks-27 → LOF: 0.8843, Label: Inlier\n","Indeks-28 → LOF: 0.8843, Label: Inlier\n","Indeks-29 → LOF: 1.0455, Label: Inlier\n","Indeks-30 → LOF: 1.0109, Label: Inlier\n","Indeks-31 → LOF: 1.6460, Label: Outlier\n","Indeks-32 → LOF: 1.2731, Label: Inlier\n","Indeks-33 → LOF: 1.1304, Label: Inlier\n","Indeks-34 → LOF: 0.9596, Label: Inlier\n","Indeks-35 → LOF: 1.2613, Label: Inlier\n","Indeks-36 → LOF: 1.3443, Label: Inlier\n","Indeks-37 → LOF: 0.9596, Label: Inlier\n","Indeks-38 → LOF: 1.1437, Label: Inlier\n","Indeks-39 → LOF: 1.0691, Label: Inlier\n","Indeks-40 → LOF: 1.0453, Label: Inlier\n","Indeks-41 → LOF: 2.6810, Label: Outlier\n","Indeks-42 → LOF: 0.9731, Label: Inlier\n","Indeks-43 → LOF: 1.0813, Label: Inlier\n","Indeks-44 → LOF: 1.3657, Label: Inlier\n","Indeks-45 → LOF: 1.1065, Label: Inlier\n","Indeks-46 → LOF: 0.9854, Label: Inlier\n","Indeks-47 → LOF: 1.0919, Label: Inlier\n","Indeks-48 → LOF: 1.1719, Label: Inlier\n","Indeks-49 → LOF: 1.1009, Label: Inlier\n","Indeks-50 → LOF: 1.0938, Label: Inlier\n","Indeks-51 → LOF: 1.0635, Label: Inlier\n","Indeks-52 → LOF: 1.1193, Label: Inlier\n","Indeks-53 → LOF: 0.9988, Label: Inlier\n","Indeks-54 → LOF: 1.0218, Label: Inlier\n","Indeks-55 → LOF: 1.2349, Label: Inlier\n","Indeks-56 → LOF: 1.1153, Label: Inlier\n","Indeks-57 → LOF: 1.0902, Label: Inlier\n","Indeks-58 → LOF: 0.9917, Label: Inlier\n","Indeks-59 → LOF: 1.5936, Label: Outlier\n","Indeks-60 → LOF: 1.0625, Label: Inlier\n","Indeks-61 → LOF: 1.3585, Label: Inlier\n","Indeks-62 → LOF: 1.7294, Label: Outlier\n","Indeks-63 → LOF: 1.0918, Label: Inlier\n","Indeks-64 → LOF: 1.6340, Label: Outlier\n","Indeks-65 → LOF: 0.9294, Label: Inlier\n","Indeks-66 → LOF: 1.2277, Label: Inlier\n","Indeks-67 → LOF: 1.1168, Label: Inlier\n","Indeks-68 → LOF: 1.1632, Label: Inlier\n","Indeks-69 → LOF: 1.0137, Label: Inlier\n","Indeks-70 → LOF: 1.0278, Label: Inlier\n","Indeks-71 → LOF: 1.1467, Label: Inlier\n","Indeks-72 → LOF: 1.0451, Label: Inlier\n","Indeks-73 → LOF: 0.9705, Label: Inlier\n","Indeks-74 → LOF: 0.9648, Label: Inlier\n","Indeks-75 → LOF: 1.0563, Label: Inlier\n","Indeks-76 → LOF: 0.9956, Label: Inlier\n","Indeks-77 → LOF: 1.0203, Label: Inlier\n","Indeks-78 → LOF: 0.9136, Label: Inlier\n","Indeks-79 → LOF: 1.2607, Label: Inlier\n","Indeks-80 → LOF: 0.9728, Label: Inlier\n","Indeks-81 → LOF: 0.9884, Label: Inlier\n","Indeks-82 → LOF: 1.0867, Label: Inlier\n","Indeks-83 → LOF: 1.0637, Label: Inlier\n","Indeks-84 → LOF: 1.3545, Label: Inlier\n","Indeks-85 → LOF: 1.2169, Label: Inlier\n","Indeks-86 → LOF: 0.9552, Label: Inlier\n","Indeks-87 → LOF: 1.2067, Label: Inlier\n","Indeks-88 → LOF: 0.9658, Label: Inlier\n","Indeks-89 → LOF: 0.9473, Label: Inlier\n","Indeks-90 → LOF: 1.1655, Label: Inlier\n","Indeks-91 → LOF: 1.0148, Label: Inlier\n","Indeks-92 → LOF: 1.1168, Label: Inlier\n","Indeks-93 → LOF: 1.1470, Label: Inlier\n","Indeks-94 → LOF: 1.1479, Label: Inlier\n","Indeks-95 → LOF: 0.9658, Label: Inlier\n","Indeks-96 → LOF: 1.0679, Label: Inlier\n","Indeks-97 → LOF: 1.0826, Label: Inlier\n","Indeks-98 → LOF: 0.9202, Label: Inlier\n","Indeks-99 → LOF: 0.9150, Label: Inlier\n","Indeks-100 → LOF: 1.2536, Label: Inlier\n","Indeks-101 → LOF: 1.0107, Label: Inlier\n","Indeks-102 → LOF: 1.1062, Label: Inlier\n","Indeks-103 → LOF: 0.9890, Label: Inlier\n","Indeks-104 → LOF: 1.0477, Label: Inlier\n","Indeks-105 → LOF: 1.0490, Label: Inlier\n","Indeks-106 → LOF: 1.8951, Label: Outlier\n","Indeks-107 → LOF: 0.9328, Label: Inlier\n","Indeks-108 → LOF: 1.6245, Label: Outlier\n","Indeks-109 → LOF: 2.0191, Label: Outlier\n","Indeks-110 → LOF: 0.9687, Label: Inlier\n","Indeks-111 → LOF: 0.9910, Label: Inlier\n","Indeks-112 → LOF: 1.0065, Label: Inlier\n","Indeks-113 → LOF: 0.9895, Label: Inlier\n","Indeks-114 → LOF: 1.5494, Label: Outlier\n","Indeks-115 → LOF: 1.1027, Label: Inlier\n","Indeks-116 → LOF: 0.9771, Label: Inlier\n","Indeks-117 → LOF: 1.2398, Label: Inlier\n","Indeks-118 → LOF: 1.1035, Label: Inlier\n","Indeks-119 → LOF: 1.1925, Label: Inlier\n","Indeks-120 → LOF: 1.0517, Label: Inlier\n","Indeks-121 → LOF: 0.9895, Label: Inlier\n","Indeks-122 → LOF: 1.1834, Label: Inlier\n","Indeks-123 → LOF: 1.0600, Label: Inlier\n","Indeks-124 → LOF: 0.9610, Label: Inlier\n","Indeks-125 → LOF: 1.0578, Label: Inlier\n","Indeks-126 → LOF: 0.9770, Label: Inlier\n","Indeks-127 → LOF: 0.9672, Label: Inlier\n","Indeks-128 → LOF: 1.0114, Label: Inlier\n","Indeks-129 → LOF: 1.0610, Label: Inlier\n","Indeks-130 → LOF: 1.0554, Label: Inlier\n","Indeks-131 → LOF: 1.3448, Label: Inlier\n","Indeks-132 → LOF: 1.0114, Label: Inlier\n","Indeks-133 → LOF: 1.0534, Label: Inlier\n","Indeks-134 → LOF: 1.5168, Label: Outlier\n","Indeks-135 → LOF: 1.1915, Label: Inlier\n","Indeks-136 → LOF: 1.0532, Label: Inlier\n","Indeks-137 → LOF: 0.9771, Label: Inlier\n","Indeks-138 → LOF: 1.0047, Label: Inlier\n","Indeks-139 → LOF: 1.0383, Label: Inlier\n","Indeks-140 → LOF: 0.9573, Label: Inlier\n","Indeks-141 → LOF: 1.0383, Label: Inlier\n","Indeks-142 → LOF: 1.0107, Label: Inlier\n","Indeks-143 → LOF: 0.9947, Label: Inlier\n","Indeks-144 → LOF: 0.9620, Label: Inlier\n","Indeks-145 → LOF: 1.0090, Label: Inlier\n","Indeks-146 → LOF: 1.1000, Label: Inlier\n","Indeks-147 → LOF: 1.0355, Label: Inlier\n","Indeks-148 → LOF: 1.0338, Label: Inlier\n","Indeks-149 → LOF: 0.9974, Label: Inlier\n"]}]},{"cell_type":"markdown","source":["**B. HASIL DARI NAIVE BAYES :**"],"metadata":{"id":"GBeIpMDgY_2l"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","filtered_data = filtered_data.copy()  # Hindari efek samping pada DataFrame asli\n","\n","filtered_data['class'] = y[lof_labels == 1].values  # Ambil label dari data asli yang bukan outlier\n","\n","filtered_data = filtered_data.reset_index(drop=True)\n","\n","X = filtered_data.drop(columns=['class'])  # Hanya fitur\n","y = filtered_data['class']  # Label\n","\n","if len(X) < 5:  # Jika data terlalu sedikit setelah filtering, tampilkan peringatan\n","    raise ValueError(\"Data terlalu sedikit setelah menghapus outliers. Coba tingkatkan nilai `n_neighbors` pada LOF.\")\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","model = GaussianNB()\n","model.fit(X_train, y_train)\n","\n","# === 5. Prediksi & Evaluasi ===\n","y_pred = model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(\"Akurasi Model Naïve Bayes setelah menghapus outliers:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NVl3AbjgY68d","executionInfo":{"status":"ok","timestamp":1743041839133,"user_tz":-420,"elapsed":98,"user":{"displayName":"23-177 Muhammad Hanif","userId":"11387158830586806132"}},"outputId":"17947006-cfdb-4739-bd5f-5b58f431463b"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Akurasi Model Naïve Bayes setelah menghapus outliers: 0.9642857142857143\n"]}]},{"cell_type":"markdown","source":["# **Berikut adalah hasil dari perhitungan pada contoh...!**"],"metadata":{"id":"3bQkXA_PaTSt"}},{"cell_type":"markdown","source":["**1. Dengan Outliers (Akurasi = 1.0)**\n","\n","Ketika model melibatkan data yang mengandung *outliers*, hasil yang diperoleh bisa sangat baik dalam konteks data uji, bahkan mencapai akurasi sempurna, yaitu **1.0**. Hal ini sering menunjukkan bahwa model sangat cocok dengan data latih yang digunakan.\n","\n","Namun, meskipun akurasi terlihat tinggi, ini bisa menjadi indikasi bahwa model telah mengalami **overfitting**. **Overfitting** terjadi ketika model \"terlalu terlatih\" pada data yang ada, termasuk noise atau data yang tidak relevan (outliers), sehingga model sangat peka terhadap data latih tersebut. Akibatnya, model ini mungkin hanya bekerja baik dengan data yang sangat mirip dengan data latih, dan tidak dapat menggeneralisasi dengan baik pada data baru yang belum pernah dilihat sebelumnya.\n","\n","Sebagai analogi, bayangkan seorang siswa yang selalu mendapatkan nilai sempurna dalam ujian karena hanya menghafal soal-soal yang sering keluar dalam ujian sebelumnya. Meskipun siswa tersebut selalu benar, dia mungkin tidak benar-benar memahami konsep di balik soal-soal tersebut. Jika diberikan soal ujian baru, yang tidak ada dalam soal yang dia hafalkan, siswa tersebut bisa saja gagal.\n","\n","**2. Tanpa Outliers (Akurasi = 0.9643 ≈ 96.4%)**\n","\n","Saat kita menghapus **outliers** dari dataset, akurasi model biasanya sedikit menurun. Dalam contoh ini, akurasi turun menjadi sekitar **96.4%**, yang menunjukkan penurunan kecil dibandingkan dengan model yang menggunakan outliers. Meskipun demikian, **penurunan akurasi ini** sebenarnya mencerminkan **kinerja yang lebih realistis** dalam dunia nyata.\n","\n","Dengan menghapus outliers, model menjadi lebih **general** dalam hal **kemampuan memprediksi data baru**. Artinya, model tidak hanya belajar pola yang ada dalam data latih, tetapi juga belajar **hubungan yang lebih luas** antara fitur-fitur tanpa terpengaruh oleh data yang mungkin tidak representatif (outliers).\n","\n","Model ini lebih stabil dan tidak terlalu terpengaruh oleh data yang ekstrem, yang biasanya tidak sesuai dengan pola umum yang ada dalam data. Dengan kata lain, model lebih cenderung untuk menemukan hubungan yang sesungguhnya antara variabel-variabel dalam data, bukan hanya sekadar \"menghafal\" pola yang terlalu spesifik dari data yang mengandung outliers.\n","\n","Sebagai analogi, ini seperti seorang siswa yang benar-benar memahami konsep ujian dan bisa mengerjakan soal-soal baru yang tidak pernah dilatih sebelumnya, meskipun soal-soal tersebut sedikit berbeda. Walaupun nilai yang didapat sedikit lebih rendah, siswa ini lebih siap menghadapi ujian yang baru.\n"],"metadata":{"id":"XPtesSFyZ1Zh"}},{"cell_type":"markdown","source":["# **Kesimpulan...!**"],"metadata":{"id":"gPlDb3aHasW6"}},{"cell_type":"markdown","source":["Berdasarkan pengamatan ini, **menghapus outliers sedikit mengurangi akurasi**, tetapi ini adalah langkah yang lebih baik dalam konteks penggunaan dunia nyata. Di dunia nyata, data tidak selalu sempurna, dan kita sering kali menghadapi data yang mengandung noise atau outliers. Dengan menghapus outliers, kita meningkatkan **kemampuan model untuk menggeneralisasi**, sehingga model lebih dapat diandalkan untuk memprediksi data yang belum pernah dilihat sebelumnya.\n","\n","Namun, **jika tujuan utama model adalah untuk mendeteksi anomali** atau mendeteksi data yang sangat berbeda dari pola normal, maka **mempertahankan outliers** bisa sangat berguna. Dalam hal ini, outliers adalah sesuatu yang ingin kita identifikasi, bukan dihilangkan.\n","\n","**Perumpamaan:**\n","- **Dengan Outlier**: Bayangkan belajar dengan kunci jawaban. Anda bisa mendapatkan nilai ujian yang sempurna, tetapi itu tidak mencerminkan pemahaman yang nyata karena Anda hanya menghafal jawaban tanpa benar-benar memahami materi. Model seperti ini hanya cocok dengan data yang mirip dengan data latih dan bisa gagal dengan data baru yang berbeda.\n","- **Tanpa Outlier**: Ini seperti belajar konsep dengan pemahaman yang dalam. Walaupun mungkin Anda tidak mendapatkan nilai sempurna setiap kali, Anda bisa mengerjakan soal-soal baru dengan baik, meskipun soal tersebut tidak pernah Anda temui sebelumnya. Model ini lebih kuat dalam memprediksi data baru, dan lebih bisa diandalkan dalam dunia nyata.\n","\n","**Pentingnya Generalisasi**:\n","Menghapus outliers penting dalam memastikan bahwa model dapat menangani **data baru** dan **beragam**. Model yang mampu menggeneralisasi dengan baik akan lebih tahan terhadap variasi yang muncul di dunia nyata, karena model tidak hanya \"menghafal\" data yang ada. Sebaliknya, model yang terlalu terlatih dengan outliers cenderung hanya dapat bekerja dengan data yang sangat mirip dengan yang telah dipelajari, yang dapat membuatnya tidak efektif ketika dihadapkan dengan data baru yang berbeda."],"metadata":{"id":"fHB3IDZtai7i"}}]}